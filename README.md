# Harvey Multimodal RAG Skeleton

This repository provides a production-ready skeleton for a multimodal retrieval-augmented generation (RAG) pipeline focused on Hurricane Harvey impact assessment in Harris County, TX.

## Quickstart

The skeleton ships with 10-row toy datasets so you can exercise the full pipeline end-to-end before loading production data. Run the commands below from the repository root unless otherwise noted.

### 1. Configure your environment

```bash
cp .env.example .env
```

Populate API keys only when you are ready to call real providers; the default `MODEL_PROVIDER=mock` keeps everything offline.

### 2. Install dependencies

If you need a fresh Python environment, create and activate one before installing:

```bash
conda create -n harvey-rag python=3.11 pip
conda activate harvey-rag
```

```bash
pip install -e .
```

This installs the FastAPI app, CLI scripts, and test utilities in editable mode so local code changes are picked up without reinstallation.

### 3. Prepare data (optional for mock mode)

* To stay in mock/demo mode, no action is requiredâ€”tests and the API will read from `data/examples/`.
* To ingest your own datasets, drop raw files into `data/raw/` and run the normalization scripts, for example:

  ```bash
  python scripts/fetch_311.py --input data/raw/houston_311.csv --output data/processed/311.parquet
  python scripts/fetch_gauges.py --input data/raw/gauges.csv --output data/processed/gauges.parquet --start 2017-08-20 --end 2017-09-10
  python scripts/index_imagery.py --input data/raw/imagery_tiles.csv --output data/processed/imagery_tiles.parquet
  ```

  Each script reports progress via `loguru` and writes Parquet/GeoParquet files plus any derived indexes under `data/processed/` and `data/indexes/`.

### 4. Run quick health checks

```bash
pytest
```

The bundled tests ensure the retrieval pipeline and FastAPI routes work against the toy data.

### 5. Launch the API server

```bash
uvicorn app.main:app --reload
```

Uvicorn will bind to `http://127.0.0.1:8000` by default.

### 6. Submit a sample query

```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"zip":"77002","start":"2017-08-28","end":"2017-09-03","k_tiles":4,"n_text":10}'
```

The response contains a structured JSON `RAGAnswer` generated by the deterministic mock model. Once you have real data and model access, flip `MODEL_PROVIDER` and rerun the query to exercise live adapters.

## Data Workflow

Place raw files in `data/raw/`. Use the scripts in `scripts/` to normalize into Parquet/GeoParquet outputs stored in `data/processed/`. Index artifacts (FAISS, H3 references) are stored in `data/indexes/`.

Example datasets with 10 toy rows are available under `data/examples/`; tests rely on these fixtures.

## Scripts

Scripts accept `--input`, `--output`, `--start`, and `--end` flags and log progress with `loguru`:

- `scripts/fetch_311.py`
- `scripts/fetch_gauges.py`
- `scripts/fetch_fema_kb.py`
- `scripts/index_imagery.py`

## Model Providers

The system supports mock, OpenAI, and Gemini providers. Configure via the `MODEL_PROVIDER` environment variable (`mock` by default). API keys are read from environment variables loaded via `.env`.

## Evaluation

Run end-to-end evaluation on example data:

```bash
python -m app.core.eval.eval_runner --config data/examples/eval_config.json
```

Outputs include metrics CSVs and confusion matrices saved under `data/processed/`.

## Testing

```bash
pytest
```

## Repository Layout

```
app/
data/
scripts/
tests/
```

Each module contains docstrings and TODOs indicating where production integrations should be implemented.
